{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiJTRgpsU1ciyH+OpMKiuh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blaydor09/AnalizadorEnriquecedorL-xico/blob/main/enriquecedorLexico_DC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìö Analizador y Enriquecedor L√©xico para Espa√±ol\n",
        "\n",
        "Este notebook permite **analizar autom√°ticamente palabras en espa√±ol** y alimentar un archivo l√©xico estructurado en formato JSON, compatible con analizadores sint√°cticos.\n",
        "\n",
        "### üöÄ ¬øQu√© hace este notebook?\n",
        "- Procesa una **lista de palabras** o un archivo `.txt` con nuevas palabras.\n",
        "- Descarga autom√°ticamente una lista con las **50,000 palabras m√°s frecuentes en espa√±ol** desde una fuente p√∫blica.\n",
        "- Elimina datos innecesarios como frecuencias de uso y conserva solo las palabras.\n",
        "- Usa **spaCy** con un modelo entrenado para espa√±ol para identificar:\n",
        "  - Tipo gramatical (verbo, sustantivo, determinante, etc.)\n",
        "  - G√©nero (masculino, femenino, neutral)\n",
        "  - N√∫mero (singular, plural)\n",
        "  - Persona, tiempo, modo, etc. (cuando corresponda)\n",
        "- Clasifica **adverbios** y **preposiciones** seg√∫n significado usando reglas personalizadas.\n",
        "- Reconoce **nombres propios** y los etiqueta con g√©nero o categor√≠a (`pa√≠s`, `ciudad`) seg√∫n el caso.\n",
        "- Actualiza autom√°ticamente el archivo principal `lexico.json`.\n",
        "- Genera un nuevo archivo `lexico_extendido.json` con solo las palabras nuevas a√±adidas.\n",
        "\n",
        "### üß† ¬øPara qui√©n es este notebook?\n",
        "Ideal para desarrolladores de:\n",
        "- Analizadores sint√°cticos en espa√±ol\n",
        "- Chatbots y asistentes de lenguaje natural\n",
        "- Sistemas educativos de gram√°tica\n",
        "- Aplicaciones ling√º√≠sticas o de procesamiento de texto\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Autor:** *Jose Fernando Nacif* ‚Äî Proyecto: *Dise√±o de Compiladores*  \n",
        "üìÖ **√öltima actualizaci√≥n:** *31/05/2025*\n"
      ],
      "metadata": {
        "id": "XSfP-LXx0GQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## === OJO ===\n",
        "### Hay que subir el archivo lexico que se esta usando en el compilador con los datos actuales...\n",
        "### En el caso de no tener el archivo usar la estructura de json de la ultima celda de este notebook. Crear el archivo lexico.json"
      ],
      "metadata": {
        "id": "36BTeCjrb_kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download es_core_news_md"
      ],
      "metadata": {
        "id": "RGyoXhSu5e6e",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc4f5d9-18ab-4f23-8815-3629f7b15546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading spacy-3.8.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, blis, thinc, spacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.11\n",
            "    Uninstalling blis-0.7.11:\n",
            "      Successfully uninstalled blis-0.7.11\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
            "langchain 0.3.19 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
            "pytensor 2.27.1 requires numpy<2,>=1.17.0, but you have numpy 2.2.6 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.3.0 numpy-2.2.6 spacy-3.8.7 thinc-8.3.6\n",
            "Collecting es-core-news-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.8.0/es_core_news_md-3.8.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Instalaci√≥n de spaCy y modelo en espa√±ol"
      ],
      "metadata": {
        "id": "Ql2lqVtkFDPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2016/es/es_50k.txt\"\n",
        "archivo_txt = \"es_50k.txt\"\n",
        "\n",
        "if not os.path.exists(archivo_txt):\n",
        "    r = requests.get(url)\n",
        "    with open(archivo_txt, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "    print(f\"‚úÖ Archivo descargado: {archivo_txt}\")\n",
        "else:\n",
        "    print(f\"üìÑ Ya existe el archivo: {archivo_txt}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8SWtV_YCgz4",
        "outputId": "c0361239-9f14-469f-a433-85affb5a13cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Archivo descargado: es_50k.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Descargar archivo de 50K palabras"
      ],
      "metadata": {
        "id": "XPDo-hSrE845"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_frecuencias(path_original, path_limpio=\"es_50k_limpio.txt\"):\n",
        "    palabras = []\n",
        "    with open(path_original, \"r\", encoding=\"utf-8\") as f_in:\n",
        "        for linea in f_in:\n",
        "            palabra = linea.strip().split(\" \")[0]\n",
        "            if palabra: palabras.append(palabra)\n",
        "\n",
        "    with open(path_limpio, \"w\", encoding=\"utf-8\") as f_out:\n",
        "        f_out.write(\"\\n\".join(palabras))\n",
        "\n",
        "    print(f\"‚úÖ Archivo limpio guardado como: {path_limpio}\")\n",
        "    return palabras\n",
        "\n",
        "lista_limpia = limpiar_frecuencias(\"es_50k.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxICh7EHCrpI",
        "outputId": "a11d8f44-5479-418c-a7a3-1edaf1e419be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Archivo limpio guardado como: es_50k_limpio.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Limpiar frecuencias y generar archivo limpio"
      ],
      "metadata": {
        "id": "lroGvozoFLld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import json\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_md\")\n",
        "\n",
        "normalizar_valores = {\n",
        "    \"Masc\": \"masculino\", \"Fem\": \"femenino\",\n",
        "    \"Sing\": \"singular\", \"Plur\": \"plural\",\n",
        "    \"1\": \"primera\", \"2\": \"segunda\", \"3\": \"tercera\",\n",
        "    \"Inf\": \"infinitivo\", \"Fin\": \"conjugado\",\n",
        "    \"empty\": \"neutral\",\n",
        "    \"Pres\": \"presente\", \"pres\": \"presente\",\n",
        "    \"Past\": \"pasado\", \"past\": \"pasado\",\n",
        "    \"Fut\": \"futuro\", \"fut\": \"futuro\",\n",
        "    \"Ind\": \"indicativo\", \"ind\": \"indicativo\",\n",
        "    \"Subj\": \"subjuntivo\", \"sub\": \"subjuntivo\", \"subj\": \"subjuntivo\",\n",
        "    \"Imp\": \"imperativo\", \"imp\": \"imperativo\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def normalizar(valor):\n",
        "    if isinstance(valor, list) and valor:\n",
        "        valor = valor[0]\n",
        "    return normalizar_valores.get(valor, valor.lower() if isinstance(valor, str) else valor)\n",
        "\n",
        "CLASIFICACION_ADVERBIOS = {\n",
        "    \"ayer\": \"tiempo\", \"hoy\": \"tiempo\", \"ma√±ana\": \"tiempo\", \"temprano\": \"tiempo\",\n",
        "    \"siempre\": \"frecuencia\", \"nunca\": \"frecuencia\", \"aqu√≠\": \"lugar\", \"all√≠\": \"lugar\",\n",
        "    \"cerca\": \"lugar\", \"lejos\": \"lugar\", \"r√°pidamente\": \"modo\", \"lentamente\": \"modo\",\n",
        "    \"bien\": \"modo\", \"mal\": \"modo\"\n",
        "}\n",
        "\n",
        "CLASIFICACION_PREPOSICIONES = {\n",
        "    \"a\": \"PREPOSICION_A\", \"de\": \"PREPOSICION_DE\", \"por\": \"PREPOSICION_POR\",\n",
        "    \"para\": \"PREPOSICION_PARA\", \"en\": \"PREPOSICION_EN\", \"con\": \"PREPOSICION_CON\",\n",
        "    \"sin\": \"PREPOSICION_SIN\", \"sobre\": \"PREPOSICION_SOBRE\", \"bajo\": \"PREPOSICION_BAJO\"\n",
        "}\n",
        "\n",
        "CLASIFICACION_NOMBRES_PROPIOS = {\n",
        "    \"argentina\": \"pa√≠s\", \"bolivia\": \"pa√≠s\", \"chile\": \"pa√≠s\", \"m√©xico\": \"pa√≠s\",\n",
        "    \"espa√±a\": \"pa√≠s\", \"francia\": \"pa√≠s\", \"la paz\": \"ciudad\", \"santa cruz\": \"ciudad\",\n",
        "    \"par√≠s\": \"ciudad\", \"madrid\": \"ciudad\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "fnQPBe0LCuLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Inicializar spaCy y definiciones gramaticales"
      ],
      "metadata": {
        "id": "mg79sTfgFRRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clasificar_token(token):\n",
        "    tag = token.pos_\n",
        "    morph = token.morph\n",
        "    info = {}\n",
        "\n",
        "    if tag == \"DET\":\n",
        "        categoria = \"determinantes\"\n",
        "        info = {\n",
        "            \"tipo\": \"DETERMINANTE\",\n",
        "            \"genero\": normalizar(morph.get(\"Gender\")) if morph.get(\"Gender\") else \"neutral\",\n",
        "            \"numero\": normalizar(morph.get(\"Number\")) if morph.get(\"Number\") else \"singular\"\n",
        "        }\n",
        "\n",
        "    elif tag == \"NOUN\":\n",
        "        categoria = \"sustantivos\"\n",
        "        info = {\n",
        "            \"tipo\": \"SUSTANTIVO\",\n",
        "            \"genero\": normalizar(morph.get(\"Gender\")) if morph.get(\"Gender\") else \"neutral\",\n",
        "            \"numero\": normalizar(morph.get(\"Number\")) if morph.get(\"Number\") else \"singular\"\n",
        "        }\n",
        "\n",
        "    elif tag in (\"VERB\", \"AUX\"):\n",
        "      categoria = \"verbos\"\n",
        "      if morph.get(\"VerbForm\") == [\"Inf\"]:\n",
        "          info = {\n",
        "              \"tipo\": \"VERBO\",\n",
        "              \"tiempo\": \"infinitivo\"\n",
        "          }\n",
        "      else:\n",
        "          info = {\n",
        "              \"tipo\": \"VERBO\",\n",
        "              \"tiempo\": normalizar(morph.get(\"Tense\")) if morph.get(\"Tense\") else \"indefinido\",\n",
        "              \"modo\": normalizar(morph.get(\"Mood\")) if morph.get(\"Mood\") else \"indefinido\",\n",
        "              \"persona\": normalizar(morph.get(\"Person\")) if morph.get(\"Person\") else \"indefinido\",\n",
        "              \"numero\": normalizar(morph.get(\"Number\")) if morph.get(\"Number\") else \"indefinido\"\n",
        "          }\n",
        "\n",
        "    elif tag == \"ADJ\":\n",
        "        categoria = \"adjetivos\"\n",
        "        info = {\n",
        "            \"tipo\": \"ADJETIVO\",\n",
        "            \"genero\": normalizar(morph.get(\"Gender\")) if morph.get(\"Gender\") else \"neutral\",\n",
        "            \"numero\": normalizar(morph.get(\"Number\")) if morph.get(\"Number\") else \"singular\"\n",
        "        }\n",
        "\n",
        "    elif tag == \"PRON\":\n",
        "        categoria = \"pronombres\"\n",
        "        info = {\n",
        "            \"tipo\": \"PRONOMBRE\",\n",
        "            \"persona\": normalizar(morph.get(\"Person\")) if morph.get(\"Person\") else \"tercera\",\n",
        "            \"numero\": normalizar(morph.get(\"Number\")) if morph.get(\"Number\") else \"singular\"\n",
        "        }\n",
        "        if morph.get(\"Gender\"):\n",
        "            info[\"genero\"] = normalizar(morph.get(\"Gender\"))\n",
        "\n",
        "    elif tag == \"PROPN\":\n",
        "        categoria = \"nombres_propios\"\n",
        "        texto_normalizado = token.text.lower()\n",
        "        info = { \"tipo\": \"NOMBRE_PROPIO\" }\n",
        "        if texto_normalizado in CLASIFICACION_NOMBRES_PROPIOS:\n",
        "            info[\"categoria\"] = CLASIFICACION_NOMBRES_PROPIOS[texto_normalizado]\n",
        "        elif morph.get(\"Gender\"):\n",
        "            info[\"genero\"] = normalizar(morph.get(\"Gender\"))\n",
        "\n",
        "    elif tag == \"ADV\":\n",
        "        categoria = \"adverbios\"\n",
        "        clase = CLASIFICACION_ADVERBIOS.get(token.text.lower(), \"modo\")\n",
        "        info = {\n",
        "            \"tipo\": \"ADVERBIO\",\n",
        "            \"clase\": clase\n",
        "        }\n",
        "\n",
        "    elif tag == \"ADP\":\n",
        "        categoria = \"preposiciones\"\n",
        "        tipo = CLASIFICACION_PREPOSICIONES.get(token.text.lower(), f\"PREPOSICION_{token.text.upper()}\")\n",
        "        info = {\n",
        "            \"tipo\": tipo\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "    return categoria, info\n"
      ],
      "metadata": {
        "id": "virGEuGGCxVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Clasificaci√≥n gramatical de palabras"
      ],
      "metadata": {
        "id": "xemr7zeUFXFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def procesar_batch(palabras, lexico, palabras_existentes, nuevas_palabras):\n",
        "    verbos_actualizados = []\n",
        "\n",
        "    for palabra in palabras:\n",
        "        doc = nlp(palabra)\n",
        "        for token in doc:\n",
        "            categoria, info = clasificar_token(token)\n",
        "            if not categoria:\n",
        "                continue\n",
        "\n",
        "            palabra_lower = palabra.lower()\n",
        "\n",
        "            if categoria == \"verbos\":\n",
        "                ya_existe = palabra_lower in palabras_existentes.get(categoria, set())\n",
        "\n",
        "                lexico[\"palabras\"].setdefault(categoria, {})[palabra_lower] = info\n",
        "                nuevas_palabras.setdefault(categoria, {})[palabra_lower] = info\n",
        "\n",
        "                if ya_existe:\n",
        "                    print(f\"üîÅ Verbo existente actualizado: '{palabra_lower}'\")\n",
        "                    verbos_actualizados.append(palabra_lower)\n",
        "\n",
        "            else:\n",
        "                if palabra_lower not in palabras_existentes.get(categoria, set()):\n",
        "                    lexico[\"palabras\"].setdefault(categoria, {})[palabra_lower] = info\n",
        "                    nuevas_palabras.setdefault(categoria, {})[palabra_lower] = info\n",
        "\n",
        "    return verbos_actualizados\n",
        "\n",
        "\n",
        "# === Leer y preparar lexico.json ===\n",
        "with open(\"lexico.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lexico = json.load(f)\n",
        "\n",
        "palabras_existentes = {cat: set(lexico[\"palabras\"].get(cat, {}).keys()) for cat in lexico[\"palabras\"]}\n",
        "nuevas_palabras = {}\n",
        "\n",
        "# === Procesar lote de palabras ===\n",
        "verbos_actualizados = procesar_batch(lista_limpia[:5000], lexico, palabras_existentes, nuevas_palabras)\n",
        "\n",
        "# === Guardar archivos ===\n",
        "with open(\"lexico.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(lexico, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "with open(\"lexico_extendido.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"palabras\": nuevas_palabras}, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "# === Resumen final ===\n",
        "total_nuevas = sum(len(v) for v in nuevas_palabras.values())\n",
        "print(f\"\\n‚úÖ {total_nuevas} palabras nuevas agregadas en {len(nuevas_palabras)} categor√≠as.\")\n",
        "print(f\"üîÅ {len(verbos_actualizados)} verbos existentes fueron actualizados.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91mLor55C0Om",
        "outputId": "9ebb23b6-23e0-49d2-a81e-bd7f9388f479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ 4924 palabras nuevas agregadas en 8 categor√≠as.\n",
            "üîÅ 0 verbos existentes fueron actualizados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Procesamiento en lote + Guardado"
      ],
      "metadata": {
        "id": "6-ayCNMpFbmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "{\n",
        "    \"palabras\": {\n",
        "        \"determinantes\": {\n",
        "\n",
        "        },\n",
        "        \"sustantivos\": {\n",
        "\n",
        "        },\n",
        "        \"verbos\": {\n",
        "\n",
        "        },\n",
        "        \"adjetivos\": {\n",
        "\n",
        "        },\n",
        "        \"pronombres\": {\n",
        "\n",
        "        },\n",
        "        \"nombres_propios\": {\n",
        "\n",
        "        },\n",
        "        \"adverbios\": {\n",
        "\n",
        "        },\n",
        "        \"preposiciones\": {\n",
        "\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R4auJm7H_3YW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}