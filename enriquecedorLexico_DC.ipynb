{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiJTRgpsU1ciyH+OpMKiuh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blaydor09/AnalizadorEnriquecedorL-xico/blob/main/enriquecedorLexico_DC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“š Analizador y Enriquecedor LÃ©xico para EspaÃ±ol\n",
        "\n",
        "Este notebook permite **analizar automÃ¡ticamente palabras en espaÃ±ol** y alimentar un archivo lÃ©xico estructurado en formato JSON, compatible con analizadores sintÃ¡cticos.\n",
        "\n",
        "### ğŸš€ Â¿QuÃ© hace este notebook?\n",
        "- Procesa una **lista de palabras** o un archivo `.txt` con nuevas palabras.\n",
        "- Descarga automÃ¡ticamente una lista con las **50,000 palabras mÃ¡s frecuentes en espaÃ±ol** desde una fuente pÃºblica.\n",
        "- Elimina datos innecesarios como frecuencias de uso y conserva solo las palabras.\n",
        "- Usa **spaCy** con un modelo entrenado para espaÃ±ol para identificar:\n",
        "  - Tipo gramatical (verbo, sustantivo, determinante, etc.)\n",
        "  - GÃ©nero (masculino, femenino, neutral)\n",
        "  - NÃºmero (singular, plural)\n",
        "  - Persona, tiempo, modo, etc. (cuando corresponda)\n",
        "- Clasifica **adverbios** y **preposiciones** segÃºn significado usando reglas personalizadas.\n",
        "- Reconoce **nombres propios** y los etiqueta con gÃ©nero o categorÃ­a (`paÃ­s`, `ciudad`) segÃºn el caso.\n",
        "- Actualiza automÃ¡ticamente el archivo principal `lexico.json`.\n",
        "- Genera un nuevo archivo `lexico_extendido.json` con solo las palabras nuevas aÃ±adidas.\n",
        "\n",
        "### ğŸ§  Â¿Para quiÃ©n es este notebook?\n",
        "Ideal para desarrolladores de:\n",
        "- Analizadores sintÃ¡cticos en espaÃ±ol\n",
        "- Chatbots y asistentes de lenguaje natural\n",
        "- Sistemas educativos de gramÃ¡tica\n",
        "- Aplicaciones lingÃ¼Ã­sticas o de procesamiento de texto\n",
        "\n",
        "---\n",
        "\n",
        "âœ… **Autor:** *Jose Fernando Nacif* â€” Proyecto: *DiseÃ±o de Compiladores*  \n",
        "ğŸ“… **Ãšltima actualizaciÃ³n:** *31/05/2025*\n"
      ],
      "metadata": {
        "id": "XSfP-LXx0GQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## === OJO ===\n",
        "### Hay que subir el archivo lexico que se esta usando en el compilador con los datos actuales...\n",
        "### En el caso de no tener el archivo usar la estructura de json de la ultima celda de este notebook. Crear el archivo lexico.json"
      ],
      "metadata": {
        "id": "36BTeCjrb_kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download es_core_news_md"
      ],
      "metadata": {
        "id": "RGyoXhSu5e6e",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc4f5d9-18ab-4f23-8815-3629f7b15546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading spacy-3.8.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, blis, thinc, spacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.11\n",
            "    Uninstalling blis-0.7.11:\n",
            "      Successfully uninstalled blis-0.7.11\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
            "langchain 0.3.19 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
            "pytensor 2.27.1 requires numpy<2,>=1.17.0, but you have numpy 2.2.6 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.3.0 numpy-2.2.6 spacy-3.8.7 thinc-8.3.6\n",
            "Collecting es-core-news-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.8.0/es_core_news_md-3.8.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-3.8.0\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> InstalaciÃ³n de spaCy y modelo en espaÃ±ol"
      ],
      "metadata": {
        "id": "Ql2lqVtkFDPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2016/es/es_50k.txt\"\n",
        "archivo_txt = \"es_50k.txt\"\n",
        "\n",
        "if not os.path.exists(archivo_txt):\n",
        "    r = requests.get(url)\n",
        "    with open(archivo_txt, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "    print(f\"âœ… Archivo descargado: {archivo_txt}\")\n",
        "else:\n",
        "    print(f\"ğŸ“„ Ya existe el archivo: {archivo_txt}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8SWtV_YCgz4",
        "outputId": "c0361239-9f14-469f-a433-85affb5a13cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Archivo descargado: es_50k.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Descargar archivo de 50K palabras"
      ],
      "metadata": {
        "id": "XPDo-hSrE845"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_frecuencias(path_original, path_limpio=\"es_50k_limpio.txt\"):\n",
        "    palabras = []\n",
        "    with open(path_original, \"r\", encoding=\"utf-8\") as f_in:\n",
        "        for linea in f_in:\n",
        "            palabra = linea.strip().split(\" \")[0]\n",
        "            if palabra: palabras.append(palabra)\n",
        "\n",
        "    with open(path_limpio, \"w\", encoding=\"utf-8\") as f_out:\n",
        "        f_out.write(\"\\n\".join(palabras))\n",
        "\n",
        "    print(f\"âœ… Archivo limpio guardado como: {path_limpio}\")\n",
        "    return palabras\n",
        "\n",
        "lista_limpia = limpiar_frecuencias(\"es_50k.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxICh7EHCrpI",
        "outputId": "a11d8f44-5479-418c-a7a3-1edaf1e419be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Archivo limpio guardado como: es_50k_limpio.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Limpiar frecuencias y generar archivo limpio"
      ],
      "metadata": {
        "id": "lroGvozoFLld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import json\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_md\")\n",
        "\n",
        "normalizar_valores = {\n",
        "    \"Masc\": \"masculino\", \"Fem\": \"femenino\",\n",
        "    \"Sing\": \"singular\", \"Plur\": \"plural\",\n",
        "    \"1\": \"primera\", \"2\": \"segunda\", \"3\": \"tercera\",\n",
        "    \"Inf\": \"infinitivo\", \"Fin\": \"conjugado\",\n",
        "    \"empty\": \"neutral\",\n",
        "    \"Pres\": \"presente\", \"pres\": \"presente\",\n",
        "    \"Past\": \"pasado\", \"past\": \"pasado\",\n",
        "    \"Fut\": \"futuro\", \"fut\": \"futuro\",\n",
        "    \"Ind\": \"indicativo\", \"ind\": \"indicativo\",\n",
        "    \"Subj\": \"subjuntivo\", \"sub\": \"subjuntivo\", \"subj\": \"subjuntivo\",\n",
        "    \"Imp\": \"imperativo\", \"imp\": \"imperativo\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def normalizar(valor):\n",
        "    if isinstance(valor, list) and valor:\n",
        "        valor = valor[0]\n",
        "    return normalizar_valores.get(valor, valor.lower() if isinstance(valor, str) else valor)\n",
        "\n",
        "CLASIFICACION_ADVERBIOS = {\n",
        "    \"ayer\": \"tiempo\", \"hoy\": \"tiempo\", \"maÃ±ana\": \"tiempo\", \"temprano\": \"tiempo\",\n",
        "    \"siempre\": \"frecuencia\", \"nunca\": \"frecuencia\", \"aquÃ­\": \"lugar\", \"allÃ­\": \"lugar\",\n",
        "    \"cerca\": \"lugar\", \"lejos\": \"lugar\", \"rÃ¡pidamente\": \"modo\", \"lentamente\": \"modo\",\n",
        "    \"bien\": \"modo\", \"mal\": \"modo\"\n",
        "}\n",
        "\n",
        "CLASIFICACION_PREPOSICIONES = {\n",
        "    \"a\": \"PREPOSICION_A\", \"de\": \"PREPOSICION_DE\", \"por\": \"PREPOSICION_POR\",\n",
        "    \"para\": \"PREPOSICION_PARA\", \"en\": \"PREPOSICION_EN\", \"con\": \"PREPOSICION_CON\",\n",
        "    \"sin\": \"PREPOSICION_SIN\", \"sobre\": \"PREPOSICION_SOBRE\", \"bajo\": \"PREPOSICION_BAJO\"\n",
        "}\n",
        "\n",
        "CLASIFICACION_NOMBRES_PROPIOS = {\n",
        "    \"argentina\": \"paÃ­s\", \"bolivia\": \"paÃ­s\", \"chile\": \"paÃ­s\", \"mÃ©xico\": \"paÃ­s\",\n",
        "    \"espaÃ±a\": \"paÃ­s\", \"francia\": \"paÃ­s\", \"la paz\": \"ciudad\", \"santa cruz\": \"ciudad\",\n",
        "    \"parÃ­s\": \"ciudad\", \"madrid\": \"ciudad\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "fnQPBe0LCuLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Inicializar spaCy y definiciones gramaticales"
      ],
      "metadata": {
        "id": "mg79sTfgFRRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clasificar_token(token):\n",
        "    tag = token.pos_\n",
        "    morph = token.morph\n",
        "    info = {}\n",
        "\n",
        "    if tag == \"DET\":\n",
        "        categoria = \"determinantes\"\n",
        "        info = {\n",
        "            \"tipo\": \"DETERMINANTE\",\n",
        "            \"genero\": normalizar(morph.get(\"Gender\")) if morph.get(\"Gender\") else \"neutral\",\n",
        "            \"numero\": normalizar(morph.get(\"Number\")) if morph.get(\"Number\") else \"singular\"\n",
        "        }\n",
        "\n",
        "    elif tag == \"NOUN\":\n",
        "        categoria = \"sustantivos\"\n",
        "        info = {\n",
        "            \"tipo\": \"SUSTANTIVO\",\n",
        "            \"genero\": normalizar(morph.get(\"Gender\")) if morph.get(\"Gender\") else \"neutral\",\n",
        "            \"numero\": normalizar(morph.get(\"Number\")) if morph.get(\"Number\") else \"singular\"\n",
        "        }\n",
        "\n",
        "    elif tag in (\"VERB\", \"AUX\"):\n",
        "      categoria = \"verbos\"\n",
        "      if morph.get(\"VerbForm\") == [\"Inf\"]:\n",
        "          info = {\n",
        "              \"tipo\": \"VERBO\",\n",
        "              \"tiempo\": \"infinitivo\"\n",
        "          }\n",
        "      else:\n",
        "          info = {\n",
        "              \"tipo\": \"VERBO\",\n",
        "              \"tiempo\": normalizar(morph.get(\"Tense\")) if morph.get(\"Tense\") else \"indefinido\",\n",
        "              \"modo\": normalizar(morph.get(\"Mood\")) if morph.get(\"Mood\") else \"indefinido\",\n",
        "              \"persona\": normalizar(morph.get(\"Person\")) if morph.get(\"Person\") else \"indefinido\",\n",
        "              \"numero\": normalizar(morph.get(\"Number\")) if morph.get(\"Number\") else \"indefinido\"\n",
        "          }\n",
        "\n",
        "    elif tag == \"ADJ\":\n",
        "        categoria = \"adjetivos\"\n",
        "        info = {\n",
        "            \"tipo\": \"ADJETIVO\",\n",
        "            \"genero\": normalizar(morph.get(\"Gender\")) if morph.get(\"Gender\") else \"neutral\",\n",
        "            \"numero\": normalizar(morph.get(\"Number\")) if morph.get(\"Number\") else \"singular\"\n",
        "        }\n",
        "\n",
        "    elif tag == \"PRON\":\n",
        "        categoria = \"pronombres\"\n",
        "        info = {\n",
        "            \"tipo\": \"PRONOMBRE\",\n",
        "            \"persona\": normalizar(morph.get(\"Person\")) if morph.get(\"Person\") else \"tercera\",\n",
        "            \"numero\": normalizar(morph.get(\"Number\")) if morph.get(\"Number\") else \"singular\"\n",
        "        }\n",
        "        if morph.get(\"Gender\"):\n",
        "            info[\"genero\"] = normalizar(morph.get(\"Gender\"))\n",
        "\n",
        "    elif tag == \"PROPN\":\n",
        "        categoria = \"nombres_propios\"\n",
        "        texto_normalizado = token.text.lower()\n",
        "        info = { \"tipo\": \"NOMBRE_PROPIO\" }\n",
        "        if texto_normalizado in CLASIFICACION_NOMBRES_PROPIOS:\n",
        "            info[\"categoria\"] = CLASIFICACION_NOMBRES_PROPIOS[texto_normalizado]\n",
        "        elif morph.get(\"Gender\"):\n",
        "            info[\"genero\"] = normalizar(morph.get(\"Gender\"))\n",
        "\n",
        "    elif tag == \"ADV\":\n",
        "        categoria = \"adverbios\"\n",
        "        clase = CLASIFICACION_ADVERBIOS.get(token.text.lower(), \"modo\")\n",
        "        info = {\n",
        "            \"tipo\": \"ADVERBIO\",\n",
        "            \"clase\": clase\n",
        "        }\n",
        "\n",
        "    elif tag == \"ADP\":\n",
        "        categoria = \"preposiciones\"\n",
        "        tipo = CLASIFICACION_PREPOSICIONES.get(token.text.lower(), f\"PREPOSICION_{token.text.upper()}\")\n",
        "        info = {\n",
        "            \"tipo\": tipo\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "    return categoria, info\n"
      ],
      "metadata": {
        "id": "virGEuGGCxVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ClasificaciÃ³n gramatical de palabras"
      ],
      "metadata": {
        "id": "xemr7zeUFXFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def procesar_batch(palabras, lexico, palabras_existentes, nuevas_palabras):\n",
        "    verbos_actualizados = []\n",
        "\n",
        "    for palabra in palabras:\n",
        "        doc = nlp(palabra)\n",
        "        for token in doc:\n",
        "            categoria, info = clasificar_token(token)\n",
        "            if not categoria:\n",
        "                continue\n",
        "\n",
        "            palabra_lower = palabra.lower()\n",
        "\n",
        "            if categoria == \"verbos\":\n",
        "                ya_existe = palabra_lower in palabras_existentes.get(categoria, set())\n",
        "\n",
        "                lexico[\"palabras\"].setdefault(categoria, {})[palabra_lower] = info\n",
        "                nuevas_palabras.setdefault(categoria, {})[palabra_lower] = info\n",
        "\n",
        "                if ya_existe:\n",
        "                    print(f\"ğŸ” Verbo existente actualizado: '{palabra_lower}'\")\n",
        "                    verbos_actualizados.append(palabra_lower)\n",
        "\n",
        "            else:\n",
        "                if palabra_lower not in palabras_existentes.get(categoria, set()):\n",
        "                    lexico[\"palabras\"].setdefault(categoria, {})[palabra_lower] = info\n",
        "                    nuevas_palabras.setdefault(categoria, {})[palabra_lower] = info\n",
        "\n",
        "    return verbos_actualizados\n",
        "\n",
        "\n",
        "# === Leer y preparar lexico.json ===\n",
        "with open(\"lexico.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lexico = json.load(f)\n",
        "\n",
        "palabras_existentes = {cat: set(lexico[\"palabras\"].get(cat, {}).keys()) for cat in lexico[\"palabras\"]}\n",
        "nuevas_palabras = {}\n",
        "\n",
        "# === Procesar lote de palabras ===\n",
        "verbos_actualizados = procesar_batch(lista_limpia[:5000], lexico, palabras_existentes, nuevas_palabras)\n",
        "\n",
        "# === Guardar archivos ===\n",
        "with open(\"lexico.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(lexico, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "with open(\"lexico_extendido.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"palabras\": nuevas_palabras}, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "# === Resumen final ===\n",
        "total_nuevas = sum(len(v) for v in nuevas_palabras.values())\n",
        "print(f\"\\nâœ… {total_nuevas} palabras nuevas agregadas en {len(nuevas_palabras)} categorÃ­as.\")\n",
        "print(f\"ğŸ” {len(verbos_actualizados)} verbos existentes fueron actualizados.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91mLor55C0Om",
        "outputId": "9ebb23b6-23e0-49d2-a81e-bd7f9388f479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… 4924 palabras nuevas agregadas en 8 categorÃ­as.\n",
            "ğŸ” 0 verbos existentes fueron actualizados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Procesamiento en lote + Guardado"
      ],
      "metadata": {
        "id": "6-ayCNMpFbmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "{\n",
        "    \"palabras\": {\n",
        "        \"determinantes\": {\n",
        "\n",
        "        },\n",
        "        \"sustantivos\": {\n",
        "\n",
        "        },\n",
        "        \"verbos\": {\n",
        "\n",
        "        },\n",
        "        \"adjetivos\": {\n",
        "\n",
        "        },\n",
        "        \"pronombres\": {\n",
        "\n",
        "        },\n",
        "        \"nombres_propios\": {\n",
        "\n",
        "        },\n",
        "        \"adverbios\": {\n",
        "\n",
        "        },\n",
        "        \"preposiciones\": {\n",
        "\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R4auJm7H_3YW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}